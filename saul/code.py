import pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as snsfrom sklearn.linear_model import LinearRegression, LogisticRegressionfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import mean_squared_error, r2_score, accuracy_score, confusion_matrix# Load the datadf = pd.read_csv("2023_MCM_Problem_C_Data.csv", skiprows=1)df = df.drop(df.columns[0], axis=1)  # Drop empty first column# Rename columns to make them easier to work withdf = df.rename(columns={    "1 try": "try1",    "2 tries": "try2",    "3 tries": "try3",    "4 tries": "try4",    "5 tries": "try5",    "6 tries": "try6",    "7 or more tries (X)": "fail"})# Calculate average tries for each worddf["avg_tries"] = (    1*df["try1"] +    2*df["try2"] +    3*df["try3"] +    4*df["try4"] +    5*df["try5"] +    6*df["try6"] +    7*df["fail"]) / 100# Create features about each worddf["unique_letters"] = df["Word"].apply(lambda x: len(set(x)))df["num_vowels"] = df["Word"].apply(lambda x: sum(c in "AEIOU" for c in x))df["has_repeated"] = df["Word"].apply(lambda x: int(len(x) != len(set(x))))# Letter frequency scores (how common each letter is in English)letter_freq = {    "E":12.7, "T":9.1, "A":8.2, "O":7.5, "I":7.0, "N":6.7, "S":6.3, "H":6.1, "R":6.0,    "D":4.3, "L":4.0, "C":2.8, "U":2.8, "M":2.4, "W":2.4, "F":2.2, "G":2.0,    "Y":2.0, "P":1.9, "B":1.3, "V":1.0, "K":0.8}df["freq_score"] = df["Word"].apply(lambda x: sum(letter_freq.get(c, 0.5) for c in x))# ===== LINEAR REGRESSION =====print("\nLINEAR REGRESSION RESULTS")# Set up features and targetfeatures = ["unique_letters", "num_vowels", "has_repeated", "freq_score"]X = df[features]y = df["avg_tries"]# Split into training and testing setsX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)# Train the modellr = LinearRegression()lr.fit(X_train, y_train)y_pred = lr.predict(X_test)# Calculate how good the model isr2 = r2_score(y_test, y_pred)rmse = np.sqrt(mean_squared_error(y_test, y_pred))print(f"RÂ² = {r2:.3f}")print(f"RMSE = {rmse:.3f}")print(f"\nCoefficients:")for feat, coef in zip(features, lr.coef_):    print(f"  {feat}: {coef:.3f}")# Plot 1: Actual vs Predictedplt.figure(figsize=(8, 6))plt.scatter(y_test, y_pred, alpha=0.6)plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)plt.xlabel("Actual Average Tries")plt.ylabel("Predicted Average Tries")plt.title("Actual vs Predicted Average Tries")plt.grid(alpha=0.3)plt.tight_layout()plt.show()# Plot 2: Feature Importanceplt.figure(figsize=(10, 6))colors = ['red' if c < 0 else 'green' for c in lr.coef_]plt.barh(features, lr.coef_, color=colors)plt.xlabel("Coefficient")plt.title("Feature Importance")plt.axvline(x=0, color='black', linestyle='-')plt.tight_layout()plt.show()# ===== CLASSIFICATION =====print("\nCLASSIFICATION RESULTS")# Create difficulty categoriesdf["difficulty"] = pd.cut(    df["avg_tries"],    bins=[0, 3.5, 4.5, 10],    labels=["Easy", "Medium", "Hard"])# Set up for classificationXc = df[features]yc = df["difficulty"]X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(Xc, yc, test_size=0.2, random_state=42)# Train classifierclf = LogisticRegression(max_iter=1000)clf.fit(X_train_c, y_train_c)y_pred_c = clf.predict(X_test_c)# Calculate accuracyaccuracy = accuracy_score(y_test_c, y_pred_c)cm = confusion_matrix(y_test_c, y_pred_c)print(f"Accuracy = {accuracy:.3f} ({accuracy*100:.1f}%)")print(f"\nConfusion Matrix:")print(cm)# Plot 3: Confusion Matrixplt.figure(figsize=(8, 6))sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',            xticklabels=["Easy", "Medium", "Hard"],            yticklabels=["Easy", "Medium", "Hard"])plt.title("Confusion Matrix")plt.xlabel("Predicted")plt.ylabel("Actual")plt.tight_layout()plt.show()